import pandas as pd
import torch
from torch import nn, optim, Tensor, zeros
# from torch.nn import function as F

# TODO: should test this.
def _index_logits(hyp_char_frame, pred_line_num_pos_i_x):
    row = hyp_char_frame.loc[(hyp_char_frame['pred_line_num'] == pred_line_num_pos_i_x[0]) & (hyp_char_frame['pos_i'] == pred_line_num_pos_i_x[1])]
    # return row.logits.values[0]
    return row.logits.values[0].unsqueeze(dim=0)

# def _map_gold_to_onehot(gold_char, char_to_i):
#     onehot = zeros(len(char_to_i))
#     gold_ind = char_to_i[gold_char]
#     onehot[gold_ind] = 1
#     return onehot

def _map_gold_to_char_i(gold_char, char_to_i):
    gold_ind = char_to_i[gold_char]
    val = Tensor([gold_ind])
    return val

class ModelWithTemperature(nn.Module):
    
    def __init__(self, hyp_char_frame, char_to_i):
        """

        Args: 
            hyp_char_frame (pd.DataFrame): |input|pos_i|pred_char|gold_char|logits|pred_line_num|
            char_to_i ({str: int}): vocabulary-index mapping generated by fairseq 
        """
        super(ModelWithTemperature, self).__init__()
        self.temperature = nn.Parameter(torch.ones(1) * 1.5)
        self.hyp_char_frame = hyp_char_frame
        self.char_to_i = char_to_i

    def forward(self, pred_line_num_pos_i_x): # 
        """

        Args:
            pred_line_num_pos_i_x ([(int, int)]): key for indexing into self.hyp_char_frame 

        Returns:
            tensor: Temperature-scaled logits
        """
        logits = _index_logits(self.hyp_char_frame, pred_line_num_pos_i_x)
        return self.temperature_scale(logits)

    def temperature_scale(self, logits):
        """Perform temperature scaling on logits.

        logits (torch.Tensor): N x 1 array 
        """
        temperature = self.temperature.unsqueeze(1).expand(logits.size(0), logits.size(1))
        return logits/temperature
    
    def set_temperature(self):
        """[summary]

        Args:
            logits (torch.Tensor): N x 1 array  
        """
        nll_criterion = nn.CrossEntropyLoss()
        # nll_criterion = nn.CrossEntropyLoss().cuda()
        labels = []
        all_logits = []
        with torch.no_grad():
            def _extract_scaled_logits(row):
                pred_line_num = row.pred_line_num
                pos_i = row.pos_i
                logits = self.forward((pred_line_num, pos_i))
                all_logits.append(logits)
                labels.append(_map_gold_to_char_i(row.gold_char, self.char_to_i))
            self.hyp_char_frame.apply(_extract_scaled_logits, axis=1)
            all_logits = torch.cat(all_logits)
            labels = torch.cat(labels).long()
        # print(all_logits.shape)
        # print(labels[0].shape)
        before_temperature_nll = nll_criterion(all_logits, labels).item()

        print(f'Before temperature optimization: {before_temperature_nll}')
        optimizer = optim.LBFGS([self.temperature], lr=0.01, max_iter=50)

        def eval():
            optimizer.zero_grad()
            loss = nll_criterion(self.temperature_scale(all_logits), labels)
            loss.backward()
            return loss
        optimizer.step(eval)

        # Calculate NLL and ECE after temperature scaling
        print('Optimal temperature: %.3f' % self.temperature.item())
        after_temperature_nll = nll_criterion(self.temperature_scale(all_logits), labels).item()
        print(f'After temperature optimization: {after_temperature_nll}')